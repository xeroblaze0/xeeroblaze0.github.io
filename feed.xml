<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://xeroblaze0.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://xeroblaze0.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-08-01T07:39:22+00:00</updated><id>https://xeroblaze0.github.io/feed.xml</id><title type="html">blank</title><subtitle>Portfolio for Alex Hay | Robotics X Neuroscience </subtitle><entry><title type="html">Design Principals for NHP Laboratory Animals</title><link href="https://xeroblaze0.github.io/blog/2022/monkey_toys/" rel="alternate" type="text/html" title="Design Principals for NHP Laboratory Animals"/><published>2022-11-21T00:00:00+00:00</published><updated>2022-11-21T00:00:00+00:00</updated><id>https://xeroblaze0.github.io/blog/2022/monkey_toys</id><content type="html" xml:base="https://xeroblaze0.github.io/blog/2022/monkey_toys/"><![CDATA[<h2 id="naxos-labs">Naxos Labs</h2> <p>In collaboration with a veterinarian, we started a company that designed a range of toys specifically tailored for NHPs and other laboratory animals. Through our experience we developed guidelines derived from valuable lessons, observations, and notes gathered during our interactions with the animals. <a href="https://naxoslab.ca/">Naxos Labs</a></p> <h3 id="purposeful-minimal-design">Purposeful Minimal Design</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/monkey/img/01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/monkey/img/01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/monkey/img/01-1400.webp"/> <img src="/assets/monkey/img/01.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/monkey/img/02-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/monkey/img/02-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/monkey/img/02-1400.webp"/> <img src="/assets/monkey/img/02.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/monkey/img/03-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/monkey/img/03-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/monkey/img/03-1400.webp"/> <img src="/assets/monkey/img/03.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The toys were designed with simplicity in mind, featuring intentionally muted and minimized elements. The use of subtle patterns and the absence of any intricate details draw the eye’s attention to outliers that serve as guides while using the toys, while the rounded corners and edges created a visually cohesive and gentle appearance. The goal was for the toys to be inviting and self-explanatory, with additional features and game rules revealed through play and exploration.</p> <div class="img_row"> <img class="col three" src="/assets/monkey/gif/00-2.gif"/> </div> <p>Likewise, we also considered the time and labor constraints of the technicians and handlers who’d also work with the toys. To address this, we added intuitive quality of life features to make the toys more user-friendly and streamline their effort.</p> <h3 id="production-matters">Production Matters</h3> <p>We opted for the Prusa Mk3S 3D printers to manufacture the toys, enabling us to rapidly iterate on designs and materials. To enhance functionality, we upgraded the printers with 32-bit Raspberry Pi control boards and firmware, enabling remote printing, multi-printer management, and advanced control options. Additionally, we equipped the printers with tungsten coated heat breaks and nozzles which offer a heat-resistant and low-friction surface as a replacement for the typical PTFE tube.</p> <p>We experimented with various materials and infills (PLA, PETG, TPU, wood, PC) but in the end landed on PLA. PETG was also on the brittle side for our use-case. ABS and ASA were contenders but PLA ended up being more compatible. PLA can be recycled, is (industrially) compostable, but ultimately won out because our vendor could verify that it was food-safe.</p> <h3 id="future-ideas">Future Ideas</h3> <p>Looking ahead there are several areas I would’ve like to have focused on. First, I would’ve developed a traceable QR code that’d be embossed on each toy. These QR codes will contain crucial traceability information such as material source, time of manufacture, batch history, satefy and material data sheets, etc.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/monkey/img/02_b-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/monkey/img/02_b-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/monkey/img/02_b-1400.webp"/> <img src="/assets/monkey/img/02_b.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/monkey/img/01_b-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/monkey/img/01_b-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/monkey/img/01_b-1400.webp"/> <img src="/assets/monkey/img/01_b.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Alternative designs for the toy underside </div> <p>Additionally, I wanted to invest in in-house recycling capabilities. By leveraging our own recycling machines we would be able to efficiently recycle toys on an individual, colony, or as per needed. Laboratories would effectively reserve a certain amount of plastic that can be shaped, cleaned, and reshaped according to their unique requirements. By providing enhanced transparency and accountability from the very beginning we provide a solid foundation for current and future scientific, industrial, and eco-political (ie. controls on plastic production and use) needs.</p>]]></content><author><name></name></author><category term="CAD"/><category term="neuroscience"/><summary type="html"><![CDATA[Naxos Labs]]></summary></entry><entry><title type="html">Microscopy and Image Analysis</title><link href="https://xeroblaze0.github.io/blog/2020/microscopy/" rel="alternate" type="text/html" title="Microscopy and Image Analysis"/><published>2020-06-09T00:00:00+00:00</published><updated>2020-06-09T00:00:00+00:00</updated><id>https://xeroblaze0.github.io/blog/2020/microscopy</id><content type="html" xml:base="https://xeroblaze0.github.io/blog/2020/microscopy/"><![CDATA[<h3 id="fiji">FIJI</h3> <p><a href="https://imagej.net/Fiji">FIJI</a> is an open source image analysis tool for the scientific community. This post demos how to use some of the tools FIJI has, including data collection techniques.</p> <h3 id="initial-data">Initial Data</h3> <div class="img_row"> <img class="col three" src="/assets/fiji/SACs_color.png"/> </div> <div class="col three caption"> Figure 1: Original image </div> <p>The image for this demo is of starburst amacrine cells (SACs), interneurons in the retina. The image is a single channel .tif, 512x512 pixels, with each pixel representing 0.62 microns. With this information we definitively measure how big each cell is.</p> <h4 id="brightness--contrast">Brightness &amp; Contrast</h4> <p>The first thing I do is adjust the brightness and contrast. By looking at the histogram we can see that the image is very dark, more dark than it needs to be. Lowering the maximum value fills out the range of present colors. The difference is clear in Figure 2.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/fiji/fig_02a-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/fiji/fig_02a-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/fiji/fig_02a-1400.webp"/> <img src="/assets/fiji/fig_02a.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/fiji/fig_02b-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/fiji/fig_02b-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/fiji/fig_02b-1400.webp"/> <img src="/assets/fiji/fig_02b.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2: left, unaltered maximum; right, fitted maximum </div> <p>It’s important in this step to not over-saturate the image by raising or lowering the minimum/maximum values. This step is to present the information within the image as a whole. Tweaking and refining will come later.</p> <p>For this demo I want to look at the cells, not the nebulous background or nodelets. Given that, the image doesn’t need to be as bright. However, it’s important to note that brightening the image revealed more cells and provided better definition. Where this middle ground is depends on the observer and whatever their interests are.</p> <h4 id="threshold">Threshold</h4> <p>Thresholding is an important first step in many image analysis techniques. It converts the image to a black and white image. Here I adjust the histogram in the same fashion as before and focus it around the peak. That filters out the background while keeping true to the cell size and shape.</p> <div class="img_row"> <img class="col three" src="/assets/fiji/fig_03.png"/> </div> <div class="col three caption"> Figure 3: Initial image after apply the threshold. Note, adjusting for a dark background flips the histogram values </div> <p>Sometimes after adjusting the image, the subject matter may blend together. In this case applying the threshold created some peanut shaped objects. Watershed segmentation addresses the issue and cuts the “peanut” in half. Figure 4 highlights the effect.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/fiji/fig_04a-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/fiji/fig_04a-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/fiji/fig_04a-1400.webp"/> <img src="/assets/fiji/fig_04a.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/fiji/fig_04b-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/fiji/fig_04b-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/fiji/fig_04b-1400.webp"/> <img src="/assets/fiji/fig_04b.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4: Left, pre-watershed; Right, post-watershed </div> <h4 id="analyzing-the-image">Analyzing the Image</h4> <p>This is where the magic happens. Once the cells are properly individualized we can count and measure each cell. FIJI automatically does this. Since I’m interested in the size and position of the cells I’ve excluded any cells that are cropped by the edge. From here the data is exported to a .csv file.</p> <div class="img_row"> <img class="col three" src="/assets/fiji/fig_05.png"/> </div> <div class="col three caption"> Figure 5: FIJI can automatically count and size each cell. </div> <h3 id="matlab">MATLAB</h3> <p>The exported data contains a cell ID, area, and x/y position. First, I want to determine if the size of the cells fits any distribution. The Jarque-Bera Test tests the normality of a dataset and MATLAB has a built in function for this. Applying the JB test to the data gives a result of 1, meaning that the test rejects the hypothesis that the data (cell size) is normally distributed.</p> <p>We can also look at the nearest neighbor of each cell (see Figure 6).</p> <div class="img_row"> <img class="col three" src="/assets/fiji/fig_06.png"/> </div> <div class="col three caption"> Figure 6: Looking at each cell's nearest neighbor </div> <p>So the size of the cells aren’t interesting, but what about the spatial distribution of the cells? Using the data provided by the image, we can calculate the coefficient of variation:</p> <p>μ = 21.0440</p> <p>σ = 5.3968</p> <p>CoV = σ/μ = 0.26</p> <p>(units are in microns)</p> <p>Defining λ as the average distance of the nearest neighbor (21.0440), the Poisson coefficient of variation is:</p> <p>CoV = λ^(-1/2) = 0.22</p> <p>A Poisson distribution assumes that cells are “blind” to each other’s positions as they develop and can’t occupy the same space. The fact that these coefficients are close infers that these SACs are also blind as they develop.</p>]]></content><author><name></name></author><category term="biology"/><category term="neuroscience"/><category term="stats"/><category term="matlab"/><summary type="html"><![CDATA[Using FIJI/ImageJ to analyze cell populations]]></summary></entry><entry><title type="html">Principal Component Analysis</title><link href="https://xeroblaze0.github.io/blog/2020/principal_component_analysis/" rel="alternate" type="text/html" title="Principal Component Analysis"/><published>2020-05-13T00:00:00+00:00</published><updated>2020-05-13T00:00:00+00:00</updated><id>https://xeroblaze0.github.io/blog/2020/principal_component_analysis</id><content type="html" xml:base="https://xeroblaze0.github.io/blog/2020/principal_component_analysis/"><![CDATA[<p> <a href=""></a><div class=""></div> <a href="https://github.com/alexanderhay2020/408/blob/master/hw/hw5/homework5.m"><div class="color-button">GitHub</div></a> </p> <h3 id="neuron-anatomy">Neuron Anatomy</h3> <p>Neurons generally have four functional regions; input, integration, conduction, and output. Inputs are generated current flowing in and out of the cell. Inputs are aggregated, and if triggered, generate an action potential and releasing neurotransmitters. In this exercise I examine the intracellular activity of a cell and determine how many presynaptic cells are providing an input, as well as the activity level of each input.</p> <div class="img_row"> <img class="col three" src="/assets/pca/fig_01.png"/> </div> <div class="col three caption"> Figure 1: General functional regions of the neuron </div> <h3 id="initial-data">Initial Data</h3> <p>To visualize the data I plotted it as a heat map. The left image shows all of the data, the right image displays fewer samples, highlighting the different inputs the cell is receiving.</p> <div class="img_row"> <img class="col three" src="/assets/pca/fig_02.png"/> </div> <div class="col three caption"> Figure 2: left, all of the sample data plotted; right, samples showing different responses </div> <p>The data is a series of voltage measurements over time; if we look at a covariance matrix (Figure 3) it would be able to show us how the voltage measured at each time point vary together.</p> <div class="img_row"> <img class="col three" src="/assets/pca/fig_03.png"/> </div> <div class="col three caption"> Figure 3: Covariance matrix of the cell voltage data </div> <h3 id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h3> <p>From there we can run PCA on the data, seen in Figure 4. Three PCs stand out in that they explain more fractional variance than the other PCs, but ultimately we would need hundreds to explain all of the data. By plotting those three principal components we can clearly see three signal responses (Figure 5, left). Looking at the histogram (Figure 5, right) confirms that the 4th PC has a gaussian centered at 0, a strong indicator of noise.</p> <div class="img_row"> <img class="col" src="/assets/pca/fig_04.png"/> </div> <div class="col three caption"> Figure 4: Principal components plotted as percent of variance explained </div> <div class="img_row" style="margin-right:1.5rem; margin-left:1.5rem;"> <img class="col two" style="float:left; padding-right: 1rem;" src="/assets/pca/fig_05_l.png"/> <img class="col two" style="float:right; padding-left: 1rem;" src="/assets/pca/fig_05_r.png"/> </div> <div class="col three caption"> Figure 5: Left, signal response of principal components 1-3; right, histogram scores </div> <h3 id="k-means-classification-and-event-identification">K-means Classification and Event Identification</h3> <p>Now that we have a waveform with which to use, we can use a classifier. In this exercise I use the K-means tool in MATLAB to comb through the data and ‘classify’ the inputs based on the PCA, counting each time a synapse event occurs. In this example each event occurred 587, 108, and 127 times respectively. There’s a number of guides and videos on how K-means works.</p> <p><img class="col three" src="/assets/pca/fig_06.png"/> </p> <div class="col three caption"> Figure 6: Results of K-means classification </div>]]></content><author><name></name></author><category term="maths"/><category term="stats"/><category term="neuroscience"/><category term="matlab"/><summary type="html"><![CDATA[Using PCA to determine number of presynaptic inputs of a cell]]></summary></entry><entry><title type="html">Function Approximation Using Radial Basis Functions</title><link href="https://xeroblaze0.github.io/blog/2020/radial_basis_functions/" rel="alternate" type="text/html" title="Function Approximation Using Radial Basis Functions"/><published>2020-01-15T00:00:00+00:00</published><updated>2020-01-15T00:00:00+00:00</updated><id>https://xeroblaze0.github.io/blog/2020/radial_basis_functions</id><content type="html" xml:base="https://xeroblaze0.github.io/blog/2020/radial_basis_functions/"><![CDATA[<p> <a href=""></a><div class=""></div> <a href="https://github.com/alexanderhay2020/469_bme/blob/master/ps1/py/part1.py"><div class="color-button">GitHub</div></a> </p> <h3 id="color-specific-photoreceptors---cones">Color Specific Photoreceptors - Cones</h3> <p>Inside the retina are cone cells, photosensitive cells that differentiate color. Humans have 3 different types of cones; (S)mall, (M)edium, and (L)arge, corresponding to the length of the wavelength that excites it. The excitement amplitudes of each type of cone is perceived to us as color, and the color perceived is the sum of each cone response.</p> <div class="img_row"> <img class="col three" src="/assets/rbf/figure_5.gif"/> </div> <div class="col three caption"> The color perceived is the sum of each cone response </div> <h3 id="radial-basis-functions">Radial Basis Functions</h3> <p>A Radial Basis Function (RBFs) is a function whose value depends the distance between a query point and a fixed point. For this exercise I used the Gaussian Function:</p> \[h(x)=exp(-\frac{(x-c^2)}{r^2})\] <ul> <li><em>x</em> is the query point</li> <li><em>c</em> is some fixed point, 0 if distance is measured from origin</li> <li><em>h(x)</em> is the RBF</li> </ul> <p>By using multiple RBFs you can approximate a function. By multiplying the RBF by some weight, summing a network of RBFs can approximate a function:</p> \[f(x) = \sum_{j=1}^{m} w_j h_j(x)\] <ul> <li><em>h(x)</em> is the RBF</li> <li><em>w</em> is the weight for the RBF</li> <li><em>j</em> in the index for <em>m</em> samples of x</li> </ul> <p>The weight vector can be found using linear regression, ultimately leading to this equation:</p> \[\overrightarrow{w} = (H^TH)^{-1}H^T\overrightarrow{y}\] <ul> <li><em>H</em> is the <em>design matrix</em> of <em>h(x)</em>, or a <em>nxm</em> matrix of <em>n</em> samples and <em>m</em> RBFs</li> <li><em>y</em> is f(x) vectorized</li> </ul> <p>In this exercise we have the following dataset:</p> <ul> <li><em>x</em> is drawn from a uniform random distribution, from <em>-10 &lt; n &lt; 10; n=1,000</em></li> <li><em>y = 2x + e</em>; e is a normally distributed noise vector, $μ = 1, σ = 0$</li> <li>Use 48 RBFs, between -12 and 12 @ every 0.5 along the x axis</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/rbf/Figure_1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/rbf/Figure_1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/rbf/Figure_1-1400.webp"/> <img src="/assets/rbf/Figure_1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/rbf/Figure_2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/rbf/Figure_2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/rbf/Figure_2-1400.webp"/> <img src="/assets/rbf/Figure_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/rbf/Figure_3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/rbf/Figure_3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/rbf/Figure_3-1400.webp"/> <img src="/assets/rbf/Figure_3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/rbf/Figure_4-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/rbf/Figure_4-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/rbf/Figure_4-1400.webp"/> <img src="/assets/rbf/Figure_4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><a href="https://alexanderhay2020.github.io/alexanderhay2020.github.io//portfolio/assets/img/Figure_1.png">Fig. 1</a> - Dataset visualized</p> <p><a href="https://alexanderhay2020.github.io/alexanderhay2020.github.io//portfolio/assets/img/Figure_2.png">Fig. 2</a> - 48 RBFs plotted</p> <p><a href="https://alexanderhay2020.github.io/alexanderhay2020.github.io//portfolio/assets/img/Figure_3.png">Fig. 3</a> - Function prediction</p> <p><a href="https://alexanderhay2020.github.io/alexanderhay2020.github.io//portfolio/assets/img/Figure_4.png">Fig. 4</a> - Error analysis</p> <p>Modeling photoreceptor response provides insight to how information is gathered and processed at the cellular level. It’s the network of these cone cells that provide the stimulus we interpret as color.</p>]]></content><author><name></name></author><category term="math"/><category term="stats"/><category term="biology"/><category term="matlab"/><summary type="html"><![CDATA[Using radial basis functions to approximate a linear mapping]]></summary></entry><entry><title type="html">Computing Logic Funcitons using Perceptrons</title><link href="https://xeroblaze0.github.io/blog/2019/perceptrons/" rel="alternate" type="text/html" title="Computing Logic Funcitons using Perceptrons"/><published>2019-11-20T00:00:00+00:00</published><updated>2019-11-20T00:00:00+00:00</updated><id>https://xeroblaze0.github.io/blog/2019/perceptrons</id><content type="html" xml:base="https://xeroblaze0.github.io/blog/2019/perceptrons/"><![CDATA[<p> <a href=""></a><div class=""></div> <a href="https://github.com/alexanderhay2020/alexanderhay2020.github.io/blob/master/portfolio/assets/py/"><div class="color-button">GitHub</div></a> </p> <h3 id="emulated-neurons">Emulated Neurons</h3> <p>Neural networks are built on units called neurons, and for this exercise a special neuron called a perceptron is used. Perceptrons are special in that they can represent fundamental logic functions: AND, OR, NAND, NOR. Though a perceptron can’t represent XAND or XOR, layered perceptrons can, thus all logic functions can potentially be built using a layered network structure.</p> <p> <img src="/assets/perceptron/img/nn_01.png" width="511" height="286" alt=""/> <br/> <em><a href="https://medium.com/@lucaspereira0612/solving-xor-with-a-single-perceptron-34539f395182">images</a> showing perceptrons' logic structure</em> </p> <p>Perceptrons work by multiplying a vector of inputs by a weight vector and passing the sum of that input-weight vectors through an activation function. For this exercise I used the sigmoid function, but there are many others. Weights are [nxm] matrices, where n is the dimension of the input and m is the dimension of the output.</p> <p> <img src="/assets/perceptron/img/nn_02.png" alt=""/> <br/> <em> image showing perceptron model</em> </p> <p><br/></p> <p>Here is a sketch algorithm to implement a perceptron node:</p> <p><br/></p> <p>\(\Sigma (x_iw_i) = x_1w_1 + x_2w_2 + ... + x_nw_n\) \(\sigma = \frac{1}{1+e^{\Sigma (x_iw_i )}}\) <br/></p> <ul> <li><em>x</em> is the sample input</li> <li><em>w</em> is the the associated weight for the input sample</li> </ul> <p>For the perceptron to work properly, the weights need to be adjusted according to the desired output. To calculate and adjust the error we first subtract the predicted output from the actual output.</p> <p>\(\epsilon=y-\sigma\) <br/></p> <ul> <li><em>ϵ</em> is the error</li> <li><em>y</em> is the acutal output</li> <li><em>σ</em> is defined above</li> </ul> <p>Using gradient descent, we find the adjustment needed for the weights by computing the derivative of the sigmoid function and multiplying that by the error to give us the final adjustment for the weights:</p> <p>\(\sigma' = \sigma (1- \sigma)\) <br/></p> <ul> <li><em>σ’</em> is the sigmoid derivative when given σ as above</li> </ul> <p>\(adjustment = \epsilon*\sigma'\) <br/></p> \[w_i=w_i+ \hat{x}^T \cdot adjustments\] <p>Networked together, perceptrons can be immensely powerful and are the foundations by which many neural nets are built. These new weights wouldn’t have changed much, but over many iterations they converge to their proper values of minimizing error. This method of adjusting the weights is called backpropagation.</p> <p>To test the algorithm a small, simple sample set was used to provide easy-to-interpret results. The table below shows the following dataset such that the output is 1 if first or second columns contained a 1, disregrading the third column:</p> <table> <thead> <tr> <th> </th> <th>Variable 1</th> <th>Variable 2</th> <th>Variable 3</th> <th>Output</th> </tr> </thead> <tbody> <tr> <td>Input 1</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> </tr> <tr> <td>Input 2</td> <td>1</td> <td>1</td> <td>1</td> <td>1</td> </tr> <tr> <td>Input 3</td> <td>1</td> <td>0</td> <td>1</td> <td>1</td> </tr> <tr> <td>Input 4</td> <td>0</td> <td>1</td> <td>1</td> <td>1</td> </tr> </tbody> </table> <p><a href="/assets/perceptron/py/perceptron.py">perceptron.py</a> demonstrates the algorithm and predicted output. Given the input array and initial weights adjusted 200​ times, the predicted results are as follows:</p> <table> <thead> <tr> <th> </th> <th>Variable 1</th> <th>Variable 2</th> <th>Variable 3</th> <th>Output</th> </tr> </thead> <tbody> <tr> <td>Input 1</td> <td>0</td> <td>0</td> <td>1</td> <td>0.135</td> </tr> <tr> <td>Input 2</td> <td>1</td> <td>1</td> <td>1</td> <td>0.999</td> </tr> <tr> <td>Input 3</td> <td>1</td> <td>0</td> <td>1</td> <td>0.917</td> </tr> <tr> <td>Input 4</td> <td>0</td> <td>1</td> <td>1</td> <td>0.917</td> </tr> </tbody> </table> <p>Given an infinite number of iterations the algorithm would converge to either 0 or 1, but in 200 iterations our results are close enough to see a clear distinction.</p> <p>Applied to a larger dataset, <a href="/assets/perceptron/py/classifier.py">classifier.py</a>, we can create a linear classifier.</p> <p> <img src="/assets/perceptron/img/Figure_2-1.png" width="50%;" height="50%;" alt=""/><img src="/assets/perceptron/img/Figure_2-2.png" width="50%;" height="50%;" alt=""/> <br/> <em>Left: Initial 2D dataset, Right: Perceptron classifier results</em> </p> <p> <img src="/assets/perceptron/img/Figure_2-4.png" width="50%;" height="50%;" alt=""/><img src="/assets/perceptron/img/Figure_2-5.png" width="50%;" height="50%;" alt=""/> <br/> <em>Left: Initial validation dataset, Right: Perceptron validation classifier results</em> </p> <p>The graph below shows the network error over 500 iterations. As expected the initial error is very high due to the weights being initially random. The error quicky drops after ~30 iterations, but never quite reaches zero. In this case error is ~4%, reflected in the misclassifed samples in both images on the right.</p> <p> <img src="/assets/perceptron/img/Figure_2-3.png" width="50%;" height="50%;" alt=""/> <br/> <em>Network error percentage drops after each epoch, indicating a model is being learned</em> </p>]]></content><author><name></name></author><category term="nerual-net"/><category term="machine-learning"/><category term="python"/><summary type="html"><![CDATA[Using layered perceptrons to compute logic functions]]></summary></entry></feed>