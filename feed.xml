<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://xeroblaze0.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://xeroblaze0.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-08-01T05:09:46+00:00</updated><id>https://xeroblaze0.github.io/feed.xml</id><title type="html">blank</title><subtitle>Portfolio for Alex Hay | Robotics X Neuroscience </subtitle><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://xeroblaze0.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://xeroblaze0.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://xeroblaze0.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Computing Logic Funcitons using Perceptrons</title><link href="https://xeroblaze0.github.io/blog/2019/perceptrons/" rel="alternate" type="text/html" title="Computing Logic Funcitons using Perceptrons"/><published>2019-11-20T00:00:00+00:00</published><updated>2019-11-20T00:00:00+00:00</updated><id>https://xeroblaze0.github.io/blog/2019/perceptrons</id><content type="html" xml:base="https://xeroblaze0.github.io/blog/2019/perceptrons/"><![CDATA[<p> <a href=""></a><div class=""></div> <a href="https://github.com/alexanderhay2020/alexanderhay2020.github.io/blob/master/portfolio/assets/py/"><div class="color-button">GitHub</div></a> </p> <h3 id="emulated-neurons">Emulated Neurons</h3> <p>Neural networks are built on units called neurons, and for this exercise a special neuron called a perceptron is used. Perceptrons are special in that they can represent fundamental logic functions: AND, OR, NAND, NOR. Though a perceptron can’t represent XAND or XOR, layered perceptrons can, thus all logic functions can potentially be built using a layered network structure.</p> <p> <img src="/assets/perceptron/nn_01.png" width="511" height="286" alt=""/> <br/> <em><a href="https://medium.com/@lucaspereira0612/solving-xor-with-a-single-perceptron-34539f395182">images</a> showing perceptrons' logic structure</em> </p> <p>Perceptrons work by multiplying a vector of inputs by a weight vector and passing the sum of that input-weight vectors through an activation function. For this exercise I used the sigmoid function, but there are many others. Weights are [nxm] matrices, where n is the dimension of the input and m is the dimension of the output.</p> <p> <img src="/assets/perceptron/nn_02.png" alt=""/> <br/> <em> image showing perceptron model</em> </p> <p><br/></p> <p>Here is a sketch algorithm to implement a perceptron node:</p> <p><br/></p> <p>\(\Sigma (x_iw_i) = x_1w_1 + x_2w_2 + ... + x_nw_n\) \(\sigma = \frac{1}{1+e^{\Sigma (x_iw_i )}}\) <br/></p> <ul> <li><em>x</em> is the sample input</li> <li><em>w</em> is the the associated weight for the input sample</li> </ul> <p>For the perceptron to work properly, the weights need to be adjusted according to the desired output. To calculate and adjust the error we first subtract the predicted output from the actual output.</p> <p>\(\epsilon=y-\sigma\) <br/></p> <ul> <li><em>ϵ</em> is the error</li> <li><em>y</em> is the acutal output</li> <li><em>σ</em> is defined above</li> </ul> <p>Using gradient descent, we find the adjustment needed for the weights by computing the derivative of the sigmoid function and multiplying that by the error to give us the final adjustment for the weights:</p> <p>\(\sigma' = \sigma (1- \sigma)\) <br/></p> <ul> <li><em>σ’</em> is the sigmoid derivative when given σ as above</li> </ul> <p>\(adjustment = \epsilon*\sigma'\) <br/></p> \[w_i=w_i+ \hat{x}^T \cdot adjustments\] <p>Networked together, perceptrons can be immensely powerful and are the foundations by which many neural nets are built. These new weights wouldn’t have changed much, but over many iterations they converge to their proper values of minimizing error. This method of adjusting the weights is called backpropagation.</p> <p>To test the algorithm a small, simple sample set was used to provide easy-to-interpret results. The table below shows the following dataset such that the output is 1 if first or second columns contained a 1, disregrading the third column:</p> <table> <thead> <tr> <th> </th> <th>Variable 1</th> <th>Variable 2</th> <th>Variable 3</th> <th>Output</th> </tr> </thead> <tbody> <tr> <td>Input 1</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> </tr> <tr> <td>Input 2</td> <td>1</td> <td>1</td> <td>1</td> <td>1</td> </tr> <tr> <td>Input 3</td> <td>1</td> <td>0</td> <td>1</td> <td>1</td> </tr> <tr> <td>Input 4</td> <td>0</td> <td>1</td> <td>1</td> <td>1</td> </tr> </tbody> </table> <p> <a href=""></a><div class=""></div> <a href="https://github.com/alexanderhay2020/alexanderhay2020.github.io/blob/master/portfolio/assets/py/perceptron.py"><div class="color-button">perceptron.py</div></a> </p> <p><a href="https://github.com/alexanderhay2020/alexanderhay2020.github.io/blob/master/portfolio/assets/py/perceptron.py">perceptron.py</a> demonstrates the algorithm and predicted output. Given the input array and initial weights adjusted 200​ times, the predicted results are as follows:</p> <table> <thead> <tr> <th> </th> <th>Variable 1</th> <th>Variable 2</th> <th>Variable 3</th> <th>Output</th> </tr> </thead> <tbody> <tr> <td>Input 1</td> <td>0</td> <td>0</td> <td>1</td> <td>0.135</td> </tr> <tr> <td>Input 2</td> <td>1</td> <td>1</td> <td>1</td> <td>0.999</td> </tr> <tr> <td>Input 3</td> <td>1</td> <td>0</td> <td>1</td> <td>0.917</td> </tr> <tr> <td>Input 4</td> <td>0</td> <td>1</td> <td>1</td> <td>0.917</td> </tr> </tbody> </table> <p>Given an infinite number of iterations the algorithm would converge to either 0 or 1, but in 200 iterations our results are close enough to see a clear distinction.</p> <p> <a href=""></a><div class=""></div> <a href="https://github.com/alexanderhay2020/alexanderhay2020.github.io/blob/master/portfolio/assets/py/classifier.py"><div class="color-button">classifier.py</div></a> </p> <p>Applied to a larger dataset, <a href="https://github.com/alexanderhay2020/alexanderhay2020.github.io/blob/master/portfolio/assets/py/classifier.py">classifier.py</a>, we can create a linear classifier.</p> <p> <img src="/assets/perceptron/Figure_2-1.png" width="50%;" height="50%;" alt=""/><img src="/assets/perceptron/Figure_2-2.png" width="50%;" height="50%;" alt=""/> <br/> <em>Left: Initial 2D dataset, Right: Perceptron classifier results</em> </p> <p> <img src="/assets/perceptron/Figure_2-4.png" width="50%;" height="50%;" alt=""/><img src="/assets/perceptron/Figure_2-5.png" width="50%;" height="50%;" alt=""/> <br/> <em>Left: Initial validation dataset, Right: Perceptron validation classifier results</em> </p> <p>The graph below shows the network error over 500 iterations. As expected the initial error is very high due to the weights being initially random. The error quicky drops after ~30 iterations, but never quite reaches zero. In this case error is ~4%, reflected in the misclassifed samples in both images on the right.</p> <p> <img src="/assets/perceptron/Figure_2-3.png" width="50%;" height="50%;" alt=""/> <br/> <em>Network error percentage drops after each epoch, indicating a model is being learned</em> </p>]]></content><author><name></name></author><category term="nerual-net"/><category term="machine-learning"/><summary type="html"><![CDATA[Using layered perceptrons to compute logic functions]]></summary></entry></feed>